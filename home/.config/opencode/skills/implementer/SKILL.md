---
name: implementer
description: Execute implementation plans by systematically completing each task with verification, bounded retry, and progress tracking
---

# Implementer Skill

## Purpose

This skill fills the workflow gap between plan creation (`/global/plan`) and PR generation (`/global/generate-pr-description`). It takes a plan file produced by the `plan-generator` skill and executes every Low-Level Task systematically — producing working code on the first pass.

The complete workflow is:

```
/global/plan → /global/implement → /global/generate-pr-description
```

## Plan Ingestion

### Locating the Plan

A plan file **must** be explicitly provided by the user. Do NOT auto-discover or assume a plan.

1. If `$ARGUMENTS` contains a file path, use that as the plan file
2. If `$ARGUMENTS` is empty or does not contain a file path, **ask the user** to provide the exact path to the plan file. Do NOT search for plans automatically.
3. Read the plan file completely

### Understanding the Plan

Plans come in different forms. Accept any of these:

- **Template plans** — Plans generated by the `plan-generator` skill with structured sections (High-Level Objective, Low-Level Tasks, Validation Gates, etc.)
- **Manual plans** — Free-form plans written by a human without a specific template
- **Ambiguous plans** — Plans with loose structure, bullet points, or minimal formatting

For **template plans** (has recognizable sections like "Low-Level Tasks", "Validation Gates"):
- Extract tasks from the "Low-Level Tasks" section
- Use "Validation Gates" as acceptance criteria
- Use "Implementation Notes" and "Context" for guidance

For **manual or ambiguous plans** (no recognizable template structure):
- Read the entire plan carefully
- Identify all actionable items, tasks, or steps described in the plan
- Infer an execution order based on dependencies and logical sequence
- If the plan is too ambiguous to extract concrete tasks, ask the user for clarification
- Treat the plan's stated goals as the acceptance criteria

### Extracting Tasks

1. Parse all tasks into an ordered list (from Low-Level Tasks section or inferred from plan content)
2. For each task, identify:
   - Target files (CREATE or UPDATE) — if specified
   - Functions to create/modify — if specified
   - Specific implementation details
   - Verification commands — if specified
3. Classify each task's status:
   - **Completed** — marked `[COMPLETED]`, `✅`, `Status: DONE`, or similar
   - **In progress** — marked `[IN PROGRESS]`, partially implemented, or has deviation notes but no completion marker
   - **Pending** — no status marker
4. Use the **TodoWrite tool** to create a todo item for each task with the correct status (`completed`, `in_progress`, or `pending`)
5. If the plan has a "Beginning context" section, read all files listed there to establish baseline understanding
6. If no "Beginning context" exists, use the task descriptions to identify which files need to be read first

### Resuming Partially Completed Plans

If the plan has any tasks marked as completed or in progress, this is a **resume**. Before implementing pending tasks:

1. **Verify completed tasks** — For each task marked completed:
   - Read the target files and confirm the expected changes actually exist
   - If a "completed" task is missing its changes (e.g., files don't exist, code wasn't written), mark it as `pending` and inform the user
   - Do NOT blindly trust completion markers — verify against the actual codebase

2. **Resume in-progress tasks** — For each task marked in progress:
   - Read the target files to understand what was partially done
   - Identify what remains to be finished
   - Continue from where it left off rather than restarting the task from scratch

3. **Run a baseline check** — Build and run unit tests to confirm the codebase is in a working state after prior changes. If it is not, report the failures to the user before continuing.

4. **Report resume status** to the user before proceeding:
   - Tasks verified complete: N
   - Tasks to resume (in progress): N
   - Tasks remaining (pending): N
   - Any completed tasks that failed verification

## Pre-Implementation Setup

Before implementing any tasks:

1. **Branch check**: Verify you are NOT on `main`. If on `main`:
   - Propose a branch name based on the plan title (kebab-case)
   - Ask the user to confirm before creating the branch
   - Create the branch with `gt create <branch-name>` if `graphite-cli` is available, otherwise use `git checkout -b <branch-name>`

2. **Baseline check**: Run the project's build/test command to confirm a green baseline
   - If the baseline is NOT green, STOP and inform the user
   - Do not proceed with implementation on a broken baseline

3. **Context loading**: Read all files in the plan's "Beginning context" section
   - Verify each file exists (or note it as expected-missing for CREATE tasks)
   - Build mental model of the codebase state before changes

## Execution Loop

For each Low-Level Task, in order:

### Step 1: Parse

- Read the task description from the plan
- Identify: target files, functions to create/update, specific details
- Mark the corresponding todo as `in_progress`

### Step 2: Context

- Read all files referenced by the task
- If files don't exist yet (CREATE tasks), verify parent directory exists
- If files exist (UPDATE tasks), read current content completely
- Understand the current state before making changes

### Step 3: Implement

Make the code changes described in the task. Follow the DOING/EXPECT pattern:

```
DOING: [what change is being made]
EXPECT: [what the code should look like after]
```

Guidelines:
- Prefer the **Edit tool** over Write for existing files
- Prefer editing existing files over creating new ones
- Follow the project's coding standards (from the plan's Implementation Notes)
- Follow the plan's specific instructions for each task
- Do NOT deviate from the plan without user approval

### Step 4: Verify

Run the verification command appropriate for the change type:

- **Code changes**: Build command (confirm compilation)
- **New tests**: Test command (confirm tests pass)
- **Style changes**: Lint command (confirm style compliance)
- **Structural changes**: Verify files exist/don't exist as expected

Compare RESULT vs EXPECT:
- If verification **passes**: proceed to Step 5
- If verification **fails**: enter the Retry Protocol (see below)

### Step 5: Record

- Update the plan file in-place: mark the task as `[COMPLETED]`
- Mark the corresponding todo as `completed`
- If there were any deviations from the plan, add a note below the task:
  ```
  > Deviation: [what changed and why]
  ```

### Step 6: Checkpoint (every 3 tasks)

- Re-read the plan's High-Level Objective
- Verify current implementation still aligns with the goal
- If drift detected: STOP, report to user, wait for confirmation before continuing

## Retry Protocol

When verification fails at Step 4:

```
attempt = 0
WHILE attempt < 3:
  1. Read the error output carefully
  2. State what failed (exact error message)
  3. State your theory of the root cause
  4. Make an adjustment to the implementation
  5. Re-run verification
  attempt++
```

If all 3 attempts fail:
- **STOP** — do not continue to the next task
- Report to user:
  1. What failed (exact error)
  2. What was attempted (all 3 approaches)
  3. Theory of root cause
  4. Proposed next step
- **Wait for user confirmation** before continuing

Critical rules:
- NEVER silently retry. Each retry must explicitly state what changed and why.
- NEVER blindly modify tests to make them pass. Follow TDD: verify that existing tests are correct and reflect the intended behavior BEFORE relying on them. If a test is wrong or the plan intentionally changes behavior, update the test with a clear justification. If a test is correct, modify code to pass it.
- NEVER modify the plan to fit broken code. Fix the code to fit the plan.

## Completion Protocol

After all Low-Level Tasks are completed:

1. **Run ALL Validation Gates** from the plan, one by one
2. **Run the full build + test + lint cycle** for the entire project
3. **Report final status** to the user:
   - Tasks completed: N/N
   - Validation gates: list each with pass/fail
   - Files created/modified/deleted (list)
   - Deviations from plan (if any)
4. **Update the plan** with a completion summary at the top:
   ```
   ## Implementation Status: COMPLETED
   Date: [current date]
   Tasks: N/N completed
   ```

## Rules

- ALWAYS update the source plan after each task (mark `[COMPLETED]`, add deviation notes)
- ALWAYS use the **TodoWrite tool** to track progress through tasks
- ALWAYS follow the DOING/EXPECT/RESULT pattern from the Agent Protocol before and after each action
- NEVER skip a task — execute them in strict order
- NEVER modify the plan's High-Level or Mid-Level Objectives
- Follow TDD: verify tests are correct before relying on them. If a test is wrong or behavior is intentionally changing, update the test with justification. If a test is correct, modify code to pass it.
- NEVER work on `main` branch — always use feature branches
- NEVER continue past a failed task without user confirmation
- NEVER deviate from the plan without user approval
- ALWAYS follow the project's coding standards from the plan's Implementation Notes
- ALWAYS prefer editing existing files over creating new ones
- ALWAYS use `dev` or `task` (go-task) commands over manual commands when available
- MAINTAINABILITY above all else
- Follow KISS (keep it simple stupid), AVOID over engineering
- If a task is ambiguous, ask the user for clarification before implementing
- Max 3 retry attempts per task before escalating to user
- Every ~10 actions, re-read the High-Level Objective (context decay prevention)
- When confused: stop, present theories, get user signoff. Uncertainty expressed > uncertainty hidden.
